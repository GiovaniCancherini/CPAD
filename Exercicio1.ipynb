{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Exercicio 01 - Coleta, Preparação e Análise de Dados\n",
    "\n",
    "### Grupo: Giovani Cancherini\n",
    "### Data de Entrega: 07/04/2024 (Turma 10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Tarefa 1 – Desenvolvendo um crawler"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Escreva um crawler para descobrir e coletar páginas da Wikipedia em Português. Seu programa deve coletar 5.000 páginas de verbetes diferentes da Wikipedia, à partir da página\n",
    "inicial: https://pt.wikipedia.org. Não coletar páginas de outros sites e também não deve\n",
    "ser coletado páginas internas da Wikipedia que não são verbetes.\n",
    "\n",
    "O crawler deve funcionar da seguinte maneira:\n",
    "1. Obter uma página.\n",
    "2. Salvar a página como um arquivo html, chamado <titulo_verbete>.html\n",
    "3. Extrair todos os links que se encontram nessa página\n",
    "4. Filtrar os links, removendo os que não se referem à verbetes e os verbetes que já foram\n",
    "visitados.\n",
    "1\n",
    "5. Guardar esses links em uma lista\n",
    "6. Escolher um link não visitado para ser a próxima página.\n",
    "7. Voltar ao passo inicial\n",
    "As páginas de verbetes coletadas deverão ser salvas como arquivos com a extensão .html.\n",
    "Lembre-se de tomar cuidado para não estressar o servidor com requisições em excesso"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import requests\n",
    "import requests\n",
    "import time \n",
    "\n",
    "# BeautifulSoup permite navegar no html\n",
    "from bs4 import BeautifulSoup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "def salvar_links(soup):\n",
    "    links_obtidos = []\n",
    "    todos_links = soup.find(id=\"bodyContent\").find_all(\"a\")\n",
    "    \n",
    "    for link in todos_links:\n",
    "        if \"href\" in link.attrs.keys() and link[\"href\"].startswith(\"/wiki/\"):\n",
    "            if link[\"href\"].find(\":\") == -1:\n",
    "                links_obtidos.append(link[\"href\"])\n",
    "                # print(links_obtidos[-1])\n",
    "                \n",
    "    return links_obtidos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "def salvar_html(nome_arquivo, response):\n",
    "    subpath = \"Exercicio1-paginas/\"\n",
    "    fullpath = f\"{subpath}{nome_arquivo}.html\"\n",
    "    with open(fullpath, \"w\", encoding=\"utf-8\") as f:\n",
    "        conteudo = response.content.decode(\"utf-8\")\n",
    "        f.write(conteudo)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "lista_paginas_visitadas = []\n",
    "lista_paginas_nao_visitadas = []\n",
    "url_base = \"https://pt.wikipedia.org\"\n",
    "\n",
    "def obtem_pagina_recursivamente(url, contador_repeticoes):\n",
    "    if contador_repeticoes == 0:\n",
    "        print()\n",
    "        print()\n",
    "        print(\"[SUCESSO]\")\n",
    "        print(\"Todas as paginas obtidas!\")\n",
    "        return\n",
    "    \n",
    "    time.sleep(1)\n",
    "    \n",
    "    response = requests.get(url)\n",
    "    lista_paginas_visitadas.append(url)\n",
    "    \n",
    "    soup = BeautifulSoup(response.content, \"html.parser\")\n",
    "    titulo = soup.select(\".mw-page-title-main\")\n",
    "    nome_arquivo = titulo[0].text.strip().replace(\" \", \"_\") if titulo else \"pagina_sem_titulo\"\n",
    "\n",
    "    links_obtidos = salvar_links(soup)\n",
    "    salvar_html(nome_arquivo, response)\n",
    "    \n",
    "    for link in links_obtidos:\n",
    "        link_completo = url_base + link\n",
    "        if link_completo not in lista_paginas_visitadas and link_completo not in lista_paginas_nao_visitadas:\n",
    "            lista_paginas_nao_visitadas.append(link_completo)\n",
    "       \n",
    "    if lista_paginas_nao_visitadas:\n",
    "        lista_paginas_nao_visitadas.pop(0)\n",
    "    else:\n",
    "        print()\n",
    "        print()\n",
    "        print(\"[ATENCAO]\")\n",
    "        print(\"Nao ha mais paginas para visitar!\")\n",
    "        \n",
    "        \n",
    "    if lista_paginas_nao_visitadas:\n",
    "        return obtem_pagina_recursivamente(lista_paginas_nao_visitadas[0], contador_repeticoes - 1)\n",
    "    else:\n",
    "        print()\n",
    "        print()\n",
    "        print(\"[ATENCAO]\")\n",
    "        print(\"Nao ha mais paginas para visitar!\")\n",
    "        return"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "[SUCESSO]\n",
      "Todas as paginas obtidas!\n"
     ]
    }
   ],
   "source": [
    "# Inicia a busca\n",
    "## Verificar output em /Exercicio1-paginas/\n",
    "\n",
    "pagina_de_inicio = \"https://pt.wikipedia.org\"\n",
    "numero_de_paginas = 50 # para teste final = 5000\n",
    "\n",
    "obtem_pagina_recursivamente(pagina_de_inicio, numero_de_paginas)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Tarefa 2 – Extraindo informações de Infoboxes"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A segunda tarefa consiste em identificar as páginas que possuem infoboxes, que são usadas\n",
    "para resumir as informações de um artigo na Wikipédia. Veja na figura a seguir um exemplo\n",
    "de página que contém um infobox (ele está destacado em vermelho na imagem).\n",
    "Infoboxes estruturam informações de diversas maneiras, por isso é difícil conseguir extrair\n",
    "todos os seus elementos de forma fácil. Portanto, focaremos nossos esforços em extrair apenas\n",
    "alguns elementos. São eles:\n",
    "1. Título: toda infobox possui um título que fica no topo da caixa. No exemplo da figura,\n",
    "o título é Alan Turing\n",
    "2. Pares chave – valor: esses pares são identificados por uma chave que está associada a\n",
    "um único valor. Por exemplo, na figura temos a chave “Nome completo” e o valor “Alan\n",
    "Mathison Turing”.\n",
    "3. Pares chave – lista: nesse tipo de item, uma chave está associada a uma lista de valores.\n",
    "Na figura de exemplo são pares de chave – lista os campos: Conhecido(a) por, Alma\n",
    "mater, Orientado(a)(s), Instituições e Campos.\n",
    "Sua tarefa consiste em extrair o conteúdo das infoboxes de todas as páginas que foram extraídas no exercício anterior. Seu programa deve identificar quando uma página possui uma\n",
    "infobox, realizar a extração das informações e salvá-las em um arquivo .json cujo nome é o\n",
    "título da infobox.\n",
    "Para fins de teste, será fornecido um conjunto de páginas juntamente com a saída esperada\n",
    "para cada uma delas.\n",
    "\n",
    "Sua tarefa consiste em extrair o conteúdo das infoboxes de todas as páginas que foram extraídas no exercício anterior. Seu programa deve identificar quando uma página possui uma\n",
    "infobox, realizar a extração das informações e salvá-las em um arquivo .json cujo nome é o\n",
    "título da infobox.\n",
    "Para fins de teste, será fornecido um conjunto de páginas juntamente com a saída esperada\n",
    "para cada uma delas.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "cpad-venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
