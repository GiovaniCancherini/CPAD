{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Exercicio 01 - Coleta, Preparação e Análise de Dados\n",
    "\n",
    "### Grupo: Giovani Cancherini\n",
    "### Data de Entrega: 07/04/2024 (Turma 10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Tarefa 1 – Desenvolvendo um crawler"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Escreva um crawler para descobrir e coletar páginas da Wikipedia em Português. Seu programa deve coletar 5.000 páginas de verbetes diferentes da Wikipedia, à partir da página\n",
    "inicial: https://pt.wikipedia.org. Não coletar páginas de outros sites e também não deve\n",
    "ser coletado páginas internas da Wikipedia que não são verbetes.\n",
    "\n",
    "O crawler deve funcionar da seguinte maneira:\n",
    "1. Obter uma página.\n",
    "2. Salvar a página como um arquivo html, chamado <titulo_verbete>.html\n",
    "3. Extrair todos os links que se encontram nessa página\n",
    "4. Filtrar os links, removendo os que não se referem à verbetes e os verbetes que já foram\n",
    "visitados.\n",
    "1\n",
    "5. Guardar esses links em uma lista\n",
    "6. Escolher um link não visitado para ser a próxima página.\n",
    "7. Voltar ao passo inicial\n",
    "As páginas de verbetes coletadas deverão ser salvas como arquivos com a extensão .html.\n",
    "Lembre-se de tomar cuidado para não estressar o servidor com requisições em excesso"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import requests\n",
    "import requests\n",
    "\n",
    "# BeautifulSoup permite navegar no html\n",
    "from bs4 import BeautifulSoup\n",
    "\n",
    "# coletando uma pagina\n",
    "url = \"https://pt.wikipedia.org/wiki/Wikip%C3%A9dia:P%C3%A1gina_principal\"\n",
    "response = requests.get(url)\n",
    "soup = BeautifulSoup(response.content, 'html.parser')\n",
    "soup.title.text\n",
    "\n",
    "# coletando multiplas paginas\n",
    "import time \n",
    "\n",
    "for _ in range(2):\n",
    "    response = requests.get(url)\n",
    "    print(\"obtive a pagina\")\n",
    "    time.sleep(1)\n",
    "\n",
    "# salvar uma pagina\n",
    "pagina = \"https://pt.wikipedia.org/wiki/Alan_Turing\"\n",
    "response = requests.get(pagina)\n",
    "soup = BeautifulSoup(response.content)\n",
    "titulo = soup.select(\".mw-page-title-main\")\n",
    "nome_arquivo = titulo[0].text\n",
    "\n",
    "subpath = \"/Exercicio1-paginas/\"\n",
    "with open(f\"{subpath}{nome_arquivo}.html\", \"w\", encoding=\"utf-8\") as f:\n",
    "    conteudo = response.content.decode(\"utf-8\")\n",
    "    f.write(conteudo)\n",
    "\n",
    "todos_links = soup.find(id=\"bodyContent\").find_all(\"a\")\n",
    "\n",
    "links = []\n",
    "for link in todos_links:\n",
    "    if \"href\" in link.attrs.keys() and link[\"href\"].startswith(\"/wiki/\"):\n",
    "        links.append(link[\"href\"])\n",
    "        print(link[-1])\n",
    "print(f\"Encontrados {len(links)} nesta pagina\")\n",
    "        \n",
    "        \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Tarefa 2 – Extraindo informações de Infoboxes"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A segunda tarefa consiste em identificar as páginas que possuem infoboxes, que são usadas\n",
    "para resumir as informações de um artigo na Wikipédia. Veja na figura a seguir um exemplo\n",
    "de página que contém um infobox (ele está destacado em vermelho na imagem).\n",
    "Infoboxes estruturam informações de diversas maneiras, por isso é difícil conseguir extrair\n",
    "todos os seus elementos de forma fácil. Portanto, focaremos nossos esforços em extrair apenas\n",
    "alguns elementos. São eles:\n",
    "1. Título: toda infobox possui um título que fica no topo da caixa. No exemplo da figura,\n",
    "o título é Alan Turing\n",
    "2. Pares chave – valor: esses pares são identificados por uma chave que está associada a\n",
    "um único valor. Por exemplo, na figura temos a chave “Nome completo” e o valor “Alan\n",
    "Mathison Turing”.\n",
    "3. Pares chave – lista: nesse tipo de item, uma chave está associada a uma lista de valores.\n",
    "Na figura de exemplo são pares de chave – lista os campos: Conhecido(a) por, Alma\n",
    "mater, Orientado(a)(s), Instituições e Campos.\n",
    "Sua tarefa consiste em extrair o conteúdo das infoboxes de todas as páginas que foram extraídas no exercício anterior. Seu programa deve identificar quando uma página possui uma\n",
    "infobox, realizar a extração das informações e salvá-las em um arquivo .json cujo nome é o\n",
    "título da infobox.\n",
    "Para fins de teste, será fornecido um conjunto de páginas juntamente com a saída esperada\n",
    "para cada uma delas.\n",
    "\n",
    "Sua tarefa consiste em extrair o conteúdo das infoboxes de todas as páginas que foram extraídas no exercício anterior. Seu programa deve identificar quando uma página possui uma\n",
    "infobox, realizar a extração das informações e salvá-las em um arquivo .json cujo nome é o\n",
    "título da infobox.\n",
    "Para fins de teste, será fornecido um conjunto de páginas juntamente com a saída esperada\n",
    "para cada uma delas.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ambiente-cpa-p3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
