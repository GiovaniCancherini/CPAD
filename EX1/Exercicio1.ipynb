{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Exercicio 01 - Coleta, Preparação e Análise de Dados\n",
    "\n",
    "### Grupo: Giovani Cancherini, Eduardo Traunig, Vinicius , João Pedro Fossa\n",
    "### Data de Entrega: 07/04/2024 (Turma 10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Tarefa 1 – Desenvolvendo um crawler"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Escreva um crawler para descobrir e coletar páginas da Wikipedia em Português. Seu programa deve coletar 5.000 páginas de verbetes diferentes da Wikipedia, à partir da página\n",
    "inicial: https://pt.wikipedia.org. Não coletar páginas de outros sites e também não deve\n",
    "ser coletado páginas internas da Wikipedia que não são verbetes.\n",
    "\n",
    "O crawler deve funcionar da seguinte maneira:\n",
    "1. Obter uma página.\n",
    "2. Salvar a página como um arquivo html, chamado <titulo_verbete>.html\n",
    "3. Extrair todos os links que se encontram nessa página\n",
    "4. Filtrar os links, removendo os que não se referem à verbetes e os verbetes que já foram\n",
    "visitados.\n",
    "1\n",
    "5. Guardar esses links em uma lista\n",
    "6. Escolher um link não visitado para ser a próxima página.\n",
    "7. Voltar ao passo inicial\n",
    "As páginas de verbetes coletadas deverão ser salvas como arquivos com a extensão .html.\n",
    "Lembre-se de tomar cuidado para não estressar o servidor com requisições em excesso"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import requests\n",
    "import requests\n",
    "import time \n",
    "\n",
    "# BeautifulSoup permite navegar no html\n",
    "from bs4 import BeautifulSoup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def salvar_links(soup):\n",
    "    links_obtidos = []\n",
    "    todos_links = soup.find(id=\"bodyContent\").find_all(\"a\")\n",
    "    \n",
    "    for link in todos_links:\n",
    "        if \"href\" in link.attrs.keys() and link[\"href\"].startswith(\"/wiki/\"):\n",
    "            if link[\"href\"].find(\":\") == -1:\n",
    "                links_obtidos.append(link[\"href\"])\n",
    "                # print(links_obtidos[-1])\n",
    "                \n",
    "    return links_obtidos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "def salvar_html(nome_arquivo, response):\n",
    "    subpath = \"Exercicio1-paginas/\"\n",
    "    fullpath = f\"{subpath}{nome_arquivo}.html\"\n",
    "    \n",
    "    os.makedirs(subpath, exist_ok=True)\n",
    "    \n",
    "    with open(fullpath, \"w\", encoding=\"utf-8\") as f:\n",
    "        conteudo = response.content.decode(\"utf-8\")\n",
    "        f.write(conteudo)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "lista_paginas_visitadas = []\n",
    "lista_paginas_nao_visitadas = []\n",
    "url_base = \"https://pt.wikipedia.org\"\n",
    "\n",
    "def obtem_pagina_recursivamente(url, contador_repeticoes):\n",
    "    if contador_repeticoes == 0:\n",
    "        print()\n",
    "        print()\n",
    "        print(\"[SUCESSO]\")\n",
    "        print(\"Todas as paginas obtidas!\")\n",
    "        return\n",
    "    \n",
    "    time.sleep(1)\n",
    "    \n",
    "    response = requests.get(url)\n",
    "    lista_paginas_visitadas.append(url)\n",
    "    \n",
    "    soup = BeautifulSoup(response.content, \"html.parser\")\n",
    "    titulo = soup.select(\".mw-page-title-main\")\n",
    "    nome_arquivo = titulo[0].text.strip().replace(\" \", \"_\") if titulo else \"pagina_sem_titulo\"\n",
    "\n",
    "    links_obtidos = salvar_links(soup)\n",
    "    salvar_html(nome_arquivo, response)\n",
    "    \n",
    "    for link in links_obtidos:\n",
    "        link_completo = url_base + link\n",
    "        if link_completo not in lista_paginas_visitadas and link_completo not in lista_paginas_nao_visitadas:\n",
    "            lista_paginas_nao_visitadas.append(link_completo)\n",
    "       \n",
    "    if lista_paginas_nao_visitadas:\n",
    "        lista_paginas_nao_visitadas.pop(0)\n",
    "    else:\n",
    "        print()\n",
    "        print()\n",
    "        print(\"[ATENCAO]\")\n",
    "        print(\"Nao ha mais paginas para visitar!\")\n",
    "        \n",
    "        \n",
    "    if lista_paginas_nao_visitadas:\n",
    "        return obtem_pagina_recursivamente(lista_paginas_nao_visitadas[0], contador_repeticoes - 1)\n",
    "    else:\n",
    "        print()\n",
    "        print()\n",
    "        print(\"[ATENCAO]\")\n",
    "        print(\"Nao ha mais paginas para visitar!\")\n",
    "        return"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "[SUCESSO]\n",
      "Todas as paginas obtidas!\n"
     ]
    }
   ],
   "source": [
    "# Inicia a busca\n",
    "## Verificar output em /Exercicio1-paginas/\n",
    "\n",
    "pagina_de_inicio = \"https://pt.wikipedia.org\"\n",
    "numero_de_paginas = 50 # para teste final = 5000\n",
    "\n",
    "obtem_pagina_recursivamente(pagina_de_inicio, numero_de_paginas)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Tarefa 2 – Extraindo informações de Infoboxes"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A segunda tarefa consiste em identificar as páginas que possuem infoboxes, que são usadas\n",
    "para resumir as informações de um artigo na Wikipédia. Veja na figura a seguir um exemplo\n",
    "de página que contém um infobox (ele está destacado em vermelho na imagem).\n",
    "Infoboxes estruturam informações de diversas maneiras, por isso é difícil conseguir extrair\n",
    "todos os seus elementos de forma fácil. Portanto, focaremos nossos esforços em extrair apenas\n",
    "alguns elementos. São eles:\n",
    "1. Título: toda infobox possui um título que fica no topo da caixa. No exemplo da figura,\n",
    "o título é Alan Turing\n",
    "2. Pares chave – valor: esses pares são identificados por uma chave que está associada a\n",
    "um único valor. Por exemplo, na figura temos a chave “Nome completo” e o valor “Alan\n",
    "Mathison Turing”.\n",
    "3. Pares chave – lista: nesse tipo de item, uma chave está associada a uma lista de valores.\n",
    "Na figura de exemplo são pares de chave – lista os campos: Conhecido(a) por, Alma\n",
    "mater, Orientado(a)(s), Instituições e Campos.\n",
    "Sua tarefa consiste em extrair o conteúdo das infoboxes de todas as páginas que foram extraídas no exercício anterior. Seu programa deve identificar quando uma página possui uma\n",
    "infobox, realizar a extração das informações e salvá-las em um arquivo .json cujo nome é o\n",
    "título da infobox.\n",
    "Para fins de teste, será fornecido um conjunto de páginas juntamente com a saída esperada\n",
    "para cada uma delas.\n",
    "\n",
    "Sua tarefa consiste em extrair o conteúdo das infoboxes de todas as páginas que foram extraídas no exercício anterior. Seu programa deve identificar quando uma página possui uma\n",
    "infobox, realizar a extração das informações e salvá-las em um arquivo .json cujo nome é o\n",
    "título da infobox.\n",
    "Para fins de teste, será fornecido um conjunto de páginas juntamente com a saída esperada\n",
    "para cada uma delas.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import json\n",
    "from bs4 import BeautifulSoup\n",
    "\n",
    "subpath = \"Exercicio1-paginas/\"\n",
    "output_path = \"Exercicio1-jsons/\"\n",
    "\n",
    "os.makedirs(output_path, exist_ok=True)\n",
    "\n",
    "def extrair_infobox(soup):\n",
    "    box = soup.find(\"table\", class_=\"infobox infobox_v2\")\n",
    "\n",
    "    if not box:\n",
    "        return None\n",
    "    info = {}\n",
    "\n",
    "    # 1. Extrair o titulo da infobox\n",
    "    titulo_tag = box.find(\"th\", class_=\"topo padrao\")\n",
    "    if titulo_tag:\n",
    "        titulo = titulo_tag.get_text(strip=True)\n",
    "        info[\"Título\"] = titulo\n",
    "    else:\n",
    "        return None\n",
    "\n",
    "    # 2. Extrair os pares chave-valor e chave-lista\n",
    "    for tag in box.find_all(\"tr\"):\n",
    "        # Procurar chave dentro de <th> ou <td scope=\"row\">\n",
    "        chave_tag = tag.find(\"th\") or tag.find(attrs={\"scope\": \"row\"})\n",
    "        if chave_tag:\n",
    "            chave = chave_tag.get_text(strip=True)\n",
    "            valores = tag.find_all(\"td\")  # Valores associados\n",
    "\n",
    "            # Se houver apenas um <td>, e um valor unico; senao, e uma lista\n",
    "            if len(valores) == 1:\n",
    "                valor = valores[0].get_text(\" | \", strip=True)\n",
    "            else:\n",
    "                valor = [td.get_text(strip=True) for td in valores]\n",
    "\n",
    "            info[chave] = valor\n",
    "\n",
    "    return info\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def salvar_json(file):\n",
    "    fullpath = os.path.join(subpath, file)\n",
    "    with open(fullpath, \"r\", encoding=\"utf-8\") as f:\n",
    "        soup = BeautifulSoup(f, \"html.parser\")\n",
    "\n",
    "    info = extrair_infobox(soup)\n",
    "\n",
    "    if info:\n",
    "        # Criar nome do arquivo JSON a partir do título da infobox\n",
    "        filename = f\"{info['Título'].replace(' ', '_')}.json\"\n",
    "        json_path = os.path.join(output_path, filename)\n",
    "\n",
    "        # Salvar os dados extraídos\n",
    "        with open(json_path, \"w\", encoding=\"utf-8\") as json_file:\n",
    "            json.dump(info, json_file, indent=4, ensure_ascii=False)\n",
    "\n",
    "        print(f\"[SUCESSO] | Informacoes extraidas e salvas                                  => {filename}\")\n",
    "    else:\n",
    "        print(f\"[ATENCAO] | {file} | Nao contem infobox\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def processar_todas_as_paginas():\n",
    "    arquivos = os.listdir(subpath)\n",
    "    \n",
    "    for file in arquivos:\n",
    "        salvar_json(file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[ATENCAO] | 1344.html | Nao contem infobox\n",
      "[ATENCAO] | 1991.html | Nao contem infobox\n",
      "[ATENCAO] | 1997.html | Nao contem infobox\n",
      "[ATENCAO] | Ataques_israelenses_na_Faixa_de_Gaza_em_março_de_2025.html | Nao contem infobox\n",
      "[ATENCAO] | Bangladesh.html | Nao contem infobox\n",
      "[ATENCAO] | Cerco_de_Algeciras_(1342–1344).html | Nao contem infobox\n",
      "[ATENCAO] | Cessar-fogo_entre_Israel_e_Hamas_em_2025.html | Nao contem infobox\n",
      "[ATENCAO] | Cláudio_Lembo.html | Nao contem infobox\n",
      "[ATENCAO] | Condado_de_San_Diego.html | Nao contem infobox\n",
      "[ATENCAO] | Congo_Belga.html | Nao contem infobox\n",
      "[ATENCAO] | Conteúdo_livre.html | Nao contem infobox\n",
      "[SUCESSO] | Informacoes extraidas e salvas                                  => Coroa_de_CastelaCorona_de_CastillaMonarquia.json\n",
      "[ATENCAO] | Cronologia_da_invasão_da_Ucrânia_pela_Rússia_(2022–presente).html | Nao contem infobox\n",
      "[ATENCAO] | Dia_da_Independência.html | Nao contem infobox\n",
      "[SUCESSO] | Informacoes extraidas e salvas                                  => Eddie_Jordan.json\n",
      "[ATENCAO] | Ekrem_İmamoğlu.html | Nao contem infobox\n",
      "[ATENCAO] | Enciclopédia.html | Nao contem infobox\n",
      "[SUCESSO] | Informacoes extraidas e salvas                                  => Enfermagem.json\n",
      "[ATENCAO] | Faixa_de_Gaza.html | Nao contem infobox\n",
      "[SUCESSO] | Informacoes extraidas e salvas                                  => Filiz_Akın.json\n",
      "[SUCESSO] | Informacoes extraidas e salvas                                  => George_ForemanCampeão_mundial_dos_pesos_pesados.json\n",
      "[ATENCAO] | Guerra_Civil_no_Sudão_(2023–presente).html | Nao contem infobox\n",
      "[ATENCAO] | Guerra_Israel-Hamas.html | Nao contem infobox\n",
      "[SUCESSO] | Informacoes extraidas e salvas                                  => Império_Merínida.json\n",
      "[ATENCAO] | Incêndio_em_boate_de_Kočani.html | Nao contem infobox\n",
      "[ATENCAO] | Invasão_da_Ucrânia_pela_Rússia_(2022–presente).html | Nao contem infobox\n",
      "[ATENCAO] | Istambul.html | Nao contem infobox\n",
      "[ATENCAO] | Jason_Sendwe.html | Nao contem infobox\n",
      "[ATENCAO] | Kirsty_Coventry.html | Nao contem infobox\n",
      "[ATENCAO] | Kočani.html | Nao contem infobox\n",
      "[ATENCAO] | Lubas.html | Nao contem infobox\n",
      "[ATENCAO] | Língua_portuguesa.html | Nao contem infobox\n",
      "[ATENCAO] | Macedónia_do_Norte.html | Nao contem infobox\n",
      "[ATENCAO] | Mercado_Comum_do_Sul.html | Nao contem infobox\n",
      "[ATENCAO] | Metodismo.html | Nao contem infobox\n",
      "[ATENCAO] | Mia_Love.html | Nao contem infobox\n",
      "[ATENCAO] | Mortes_em_2025.html | Nao contem infobox\n",
      "[SUCESSO] | Informacoes extraidas e salvas                                  => Heaven's_Gate.json\n",
      "[SUCESSO] | Informacoes extraidas e salvas                                  => Político.json\n",
      "[ATENCAO] | Presidente_do_Comitê_Olímpico_Internacional.html | Nao contem infobox\n",
      "[ATENCAO] | Protestos_na_Turquia_em_2025.html | Nao contem infobox\n",
      "[ATENCAO] | Página_principal.html | Nao contem infobox\n",
      "[ATENCAO] | Reconquista.html | Nao contem infobox\n",
      "[ATENCAO] | República_Democrática_do_Congo.html | Nao contem infobox\n",
      "[ATENCAO] | República_do_Congo_(Léopoldville).html | Nao contem infobox\n",
      "[ATENCAO] | Seita.html | Nao contem infobox\n",
      "[ATENCAO] | Tanganhica_(República_Democrática_do_Congo).html | Nao contem infobox\n",
      "[ATENCAO] | Tratado_de_Assunção.html | Nao contem infobox\n",
      "[ATENCAO] | Turquia.html | Nao contem infobox\n",
      "[ATENCAO] | Wlamir_Marques.html | Nao contem infobox\n"
     ]
    }
   ],
   "source": [
    "# Inicia a busca\n",
    "## Verificar output em /Exercicio1-jsons/\n",
    "\n",
    "processar_todas_as_paginas()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "cpad-venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
